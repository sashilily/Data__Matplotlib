{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294cb6d6-a249-4b74-86a4-d4316766d133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcfffd",
   "metadata": {},
   "source": [
    "Implementiong a simple GAN using TensorFlow/Keras to generate handwritten digits simila t those in MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed73acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf5d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f370b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = (X_train.astype(np.float32)- 127.5)/ 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e4183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62cdc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defeine the generator model\n",
    "generator = Sequential([\n",
    "    Dense(256, input_dim=100),\n",
    "    LeakyReLU(negative_slope = 0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(512),\n",
    "    LeakyReLU(negative_slope=0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(1024),\n",
    "    LeakyReLU(negative_slope=0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(784, activation='tanh'),\n",
    "    Reshape((28,28))\n",
    "]) ## I had a proble using álpha' since the error states that it's highly depreciated, therfore I used 'negative_slope'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b33b9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "discriminator = Sequential([\n",
    "    Flatten(input_shape = (28,28)),\n",
    "    Dense(1024),\n",
    "    LeakyReLU(negative_slope = 0.2),\n",
    "    Dense(256),\n",
    "    LeakyReLU(negative_slope = 0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6eba2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer = Adam(learning_rate = 0.0002,\n",
    "                                      beta_1 = 0.5),\n",
    "                     loss = 'binary+crossentropy',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d7c50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "x= generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer = Adam(learning_rate = 0.0002, beta_1 = 0.5),\n",
    "           loss ='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8ee552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the GAN \n",
    "def train_gan(epochs=1, batch_size=128):\n",
    "    #Calculate the number of batches per epoch\n",
    "    batch_count = X_train.shape[0] // batch_size\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            #Generate random noise as input generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            \n",
    "            #Generate fake images using the generator\n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            #Get a random batch of real images from the dataset\n",
    "            batch_idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_images = X_train[batch_idx]\n",
    "            \n",
    "            #Concatenate real and fake images\n",
    "            X = np.concatenate([real_images, generated_images])\n",
    "            \n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros( 2* batch_size)\n",
    "            y_dis[:batch_size] = 0.9 # One-sided label smoothing\n",
    "            \n",
    "            #Train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            # Train the generator ( via the GAN model)\n",
    "            noise= np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "            #print the progress and save the generated images\n",
    "            print(f\"Epoch {e+1},Discriminator Loss: {d_loss[0]}, Generator Loss: {(g_loss)}\")\n",
    "            \n",
    "            if e% 10 == 0:\n",
    "                plot_generated_images(e, generator)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b645709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "real_images shape: (128, 784)\n",
      "generated_images shape: (128, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(256, 784), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (256, 784)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(256, 784), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m train_gan(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 30\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(epochs, batch_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the discriminator\u001b[39;00m\n\u001b[0;32m     29\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(X, y_dis)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the generator\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Generate a batch of noise\u001b[39;00m\n\u001b[0;32m     35\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:544\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 544\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[0;32m    545\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    125\u001b[0m     outputs,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:51\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_has_training_arg:\n\u001b[1;32m---> 51\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(256, 784), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (256, 784)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(256, 784), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "def train_gan(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_size):\n",
    "            # Train the discriminator\n",
    "\n",
    "            # Select a random batch of real images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_images = X_train[idx]\n",
    "\n",
    "            # Generate a batch of fake images\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Reshape generated images to match real images shape\n",
    "            generated_images = generated_images.reshape(batch_size, -1)\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            print(f\"real_images shape: {real_images.shape}\")\n",
    "            print(f\"generated_images shape: {generated_images.shape}\")\n",
    "\n",
    "            # Concatenate real and fake images\n",
    "            X = np.concatenate([real_images, generated_images])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 1\n",
    "\n",
    "            # Train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train the generator\n",
    "\n",
    "            # Generate a batch of noise\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Labels for fake data to mislead the discriminator\n",
    "            y_gen = np.ones(batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f\"{epoch}/{epochs} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=100, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d1897a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot generated images\n",
    "def plot_generated_images(epoch, generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "    noise = np.random.normal(0, 1, size =[examples, 100])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "    \n",
    "    plt.figure(figsize = figsize)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[1],\n",
    "                  interpolation='nearest', cmap ='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.saveig(f'gan_generated_image_epoch_{epoch}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82dd722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot get result() since the metric has not yet been built.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_gan(epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(epochs, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m y_gen \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(batch_size)\n\u001b[0;32m     32\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m gan\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, y_gen)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#print the progress and save the generated images\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#print(f\"Epoch {e+1},Discriminator Loss: {d_loss[0]}, Generator Loss: {(g_loss)}\")\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:544\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 544\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[0;32m    545\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    125\u001b[0m     outputs,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:77\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model does not have any trainable weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:445\u001b[0m, in \u001b[0;36mTrainer.compute_metrics\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_metrics\u001b[38;5;241m.\u001b[39mupdate_state(y, y_pred, sample_weight)\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:459\u001b[0m, in \u001b[0;36mTrainer.get_metrics_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m return_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m--> 459\u001b[0m     result \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    461\u001b[0m         return_metrics\u001b[38;5;241m.\u001b[39mupdate(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:356\u001b[0m, in \u001b[0;36mCompileMetrics.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get result() since the metric has not yet been built.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         )\n\u001b[0;32m    359\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    360\u001b[0m     unique_name_counters \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot get result() since the metric has not yet been built."
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "train_gan(epochs= 100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cacea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1775f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_2, built=True>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00573a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_lower(_dict, _string):\n",
    "  \"\"\"Add a mapping between `_string` and a lowercased version of `_string` to `_dict`\n",
    "\n",
    "  Args:\n",
    "    _dict (dict): The dictionary to update.\n",
    "    _string (str): The string to add.\n",
    "  \"\"\"\n",
    "  orig_string = _string\n",
    "  _string = _string.lower()\n",
    "  _dict[orig_string] = _string\n",
    "\n",
    "d = {}\n",
    "s = 'Hello'\n",
    "\n",
    "store_lower(d, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b20f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 'hello'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebba798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a783ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "x = 50\n",
    "\n",
    "def one(y):\n",
    "  x = 10\n",
    "\n",
    "def two(y):\n",
    "  global x\n",
    "  x = 30\n",
    "\n",
    "def three(y):\n",
    "  x = 100\n",
    "  print(x)\n",
    "\n",
    "for func in [one, two, three]:\n",
    "  func(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f446b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before multiply\n",
      "50\n",
      "After multiply\n"
     ]
    }
   ],
   "source": [
    "def print_before_and_after(func):\n",
    "  \"\"\"\n",
    "  defines a nested function wrapper() that calls whatever function gets passed to print_before_and_after()\n",
    "  \"\"\"\n",
    "  def wrapper(*args):\n",
    "    print('Before {}'.format(func.__name__))\n",
    "    # Call the function being decorated with *args\n",
    "    func(*args)\n",
    "    print('After {}'.format(func.__name__))\n",
    "  # Return the nested function\n",
    "  return wrapper\n",
    "\n",
    "\n",
    "@print_before_and_after\n",
    "def multiply(a, b):\n",
    "  print(a * b)\n",
    "\n",
    "multiply(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35611902",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack_overflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(stack_overflow)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stack_overflow' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(stack_overflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07297e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cryptography as cpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b0d2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cryptography' has no attribute 'funnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cpt\u001b[38;5;241m.\u001b[39mfunnet()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cryptography' has no attribute 'funnet'"
     ]
    }
   ],
   "source": [
    "cpt.funnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1f173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package cryptography:\n",
      "\n",
      "NAME\n",
      "    cryptography\n",
      "\n",
      "DESCRIPTION\n",
      "    # This file is dual licensed under the terms of the Apache License, Version\n",
      "    # 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "    # for complete details.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __about__\n",
      "    exceptions\n",
      "    fernet\n",
      "    hazmat (package)\n",
      "    utils\n",
      "    x509 (package)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['__version__', '__author__', '__copyright__']\n",
      "    __copyright__ = 'Copyright 2013-2024 The Python Cryptographic Authorit...\n",
      "\n",
      "VERSION\n",
      "    42.0.2\n",
      "\n",
      "AUTHOR\n",
      "    The Python Cryptographic Authority and individual contributors\n",
      "\n",
      "FILE\n",
      "    c:\\users\\user\\anaconda3\\lib\\site-packages\\cryptography\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3323b946",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cryptography' has no attribute 'fernet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cpt\u001b[38;5;241m.\u001b[39mfernet()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cryptography' has no attribute 'fernet'"
     ]
    }
   ],
   "source": [
    "cpt.fernet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49faf201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'A really secret message. Not for prying eyes.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cryptography.fernet import Fernet\n",
    "# Put this somewhere safe!\n",
    "key = Fernet.generate_key()\n",
    "f = Fernet(key)\n",
    "token = f.encrypt(b\"A really secret message. Not for prying eyes.\")\n",
    "token\n",
    "b'...'\n",
    "f.decrypt(token)\n",
    "b'A really secret message. Not for prying eyes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe533bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
